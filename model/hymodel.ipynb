{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fH2tcXhAvx_l",
        "outputId": "2a572800-4d10-437f-c8ea-83e0cf0e9043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.5.0\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425346 sha256=3ba2e844aff220ba186ddcec0426bb6520891332c392448ee2afc32260c92697\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/df/61/8c121f50c3cffd77f8178180dd232d90b3b99d1bd61fb6d6be\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "  Attempting uninstall: pyspark\n",
            "    Found existing installation: pyspark 3.5.5\n",
            "    Uninstalling pyspark-3.5.5:\n",
            "      Successfully uninstalled pyspark-3.5.5\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "515259d493be44fda02eeee727301aeb",
              "pip_warning": {
                "packages": [
                  "pyspark"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting snowflake-connector-python\n",
            "  Downloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/67.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting asn1crypto<2.0.0,>0.24.0 (from snowflake-connector-python)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.1)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (24.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.12.2)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python) (4.3.7)\n",
            "Collecting tomlkit (from snowflake-connector-python)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->snowflake-connector-python) (2.3.0)\n",
            "Downloading snowflake_connector_python-3.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.0/105.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: asn1crypto, tomlkit, snowflake-connector-python\n",
            "Successfully installed asn1crypto-1.5.1 snowflake-connector-python-3.14.0 tomlkit-0.13.2\n",
            "Collecting snowflake-sqlalchemy\n",
            "  Downloading snowflake_sqlalchemy-1.7.3-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: snowflake-connector-python<4.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-sqlalchemy) (3.14.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.19 in /usr/local/lib/python3.11/dist-packages (from snowflake-sqlalchemy) (2.0.39)\n",
            "Requirement already satisfied: asn1crypto<2.0.0,>0.24.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.5.1)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.9 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (43.0.3)\n",
            "Requirement already satisfied: pyOpenSSL<26.0.0,>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (24.2.1)\n",
            "Requirement already satisfied: pyjwt<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.10.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2025.1)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.32.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (24.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2025.1.31)\n",
            "Requirement already satisfied: typing_extensions<5,>=4.3 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (4.12.2)\n",
            "Requirement already satisfied: filelock<4,>=3.5 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.4.0)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (4.3.7)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (0.13.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.19->snowflake-sqlalchemy) (3.1.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi<2.0.0,>=1.9->snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0->snowflake-connector-python<4.0.0->snowflake-sqlalchemy) (2.3.0)\n",
            "Downloading snowflake_sqlalchemy-1.7.3-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: snowflake-sqlalchemy\n",
            "Successfully installed snowflake-sqlalchemy-1.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5.0\n",
        "!pip install snowflake-connector-python\n",
        "!pip install snowflake-sqlalchemy\n",
        "!pip install -q findspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rASlXLWAve4R"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ExchangeRatePrediction\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCasND8ZxANT",
        "outputId": "bb4b39ec-ec9b-49d9-9b0e-a0ad6d57ec2b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-13b0d0ad704f>:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
            "  df_pd = pd.read_sql(query, conn)\n"
          ]
        }
      ],
      "source": [
        "import snowflake.connector\n",
        "import pandas as pd\n",
        "\n",
        "conn = snowflake.connector.connect(\n",
        "    user=\"###\",\n",
        "    password=\"###\",\n",
        "    account=\"####\",\n",
        "    warehouse=\"###\",\n",
        "    database=\"EXCHANGE_RATES_SL\",\n",
        "    schema=\"DW\"\n",
        ")\n",
        "\n",
        "query = \"SELECT * FROM PREDICTION_FEATURES\"\n",
        "df_pd = pd.read_sql(query, conn)\n",
        "conn.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3FJDBbpxE-8",
        "outputId": "b07c1432-c6c5-480e-b1db-fe250a2f6002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- DATE: date (nullable = true)\n",
            " |-- USD: double (nullable = true)\n",
            " |-- EUR: double (nullable = true)\n",
            " |-- GBP: double (nullable = true)\n",
            " |-- INR: double (nullable = true)\n",
            " |-- RUB: double (nullable = true)\n",
            " |-- CNY: double (nullable = true)\n",
            " |-- AUD: double (nullable = true)\n",
            " |-- TOURISM_SL: double (nullable = true)\n",
            " |-- SRI_LANKA_GDP_USD_BILLIONS_USD: double (nullable = true)\n",
            " |-- ANNUAL_CHANGE_PCR: double (nullable = true)\n",
            " |-- PER_CAPITA_USD: double (nullable = true)\n",
            " |-- MERCHANDISE_EXPORTS_MILLIONS_USD: double (nullable = true)\n",
            " |-- LABOUR_MIGRATION_SL: double (nullable = true)\n",
            " |-- MERCHANDISE_IMPORTS_MILLIONS_USD: double (nullable = true)\n",
            " |-- INFLATION_RATE_PCR: double (nullable = true)\n",
            " |-- GDP_PER_CAPITA_USD: double (nullable = true)\n",
            " |-- GDP_GROWTH_PCT: double (nullable = true)\n",
            "\n",
            "+----------+--------+--------+--------+------+------+-------+--------+----------+------------------------------+-----------------+--------------+--------------------------------+-------------------+--------------------------------+------------------+------------------+--------------+\n",
            "|      DATE|     USD|     EUR|     GBP|   INR|   RUB|    CNY|     AUD|TOURISM_SL|SRI_LANKA_GDP_USD_BILLIONS_USD|ANNUAL_CHANGE_PCR|PER_CAPITA_USD|MERCHANDISE_EXPORTS_MILLIONS_USD|LABOUR_MIGRATION_SL|MERCHANDISE_IMPORTS_MILLIONS_USD|INFLATION_RATE_PCR|GDP_PER_CAPITA_USD|GDP_GROWTH_PCT|\n",
            "+----------+--------+--------+--------+------+------+-------+--------+----------+------------------------------+-----------------+--------------+--------------------------------+-------------------+--------------------------------+------------------+------------------+--------------+\n",
            "|2010-05-07|113.6975|144.1798| 168.585| 2.506|3.7118|16.6547|101.1737|       NaN|                        6.3899|           0.0197|        1.9733|                       9293.0822|        265937.6959|                      15804.4764|            0.0639|           2978.88|        8.2416|\n",
            "|2010-05-11|113.6292|145.3033|168.7962|2.5406|3.7332|16.6431| 102.539|       NaN|                        6.3954|           0.0195|        1.9486|                       9314.2658|        265887.8767|                      15879.1932|             0.064|           2983.38|        8.2487|\n",
            "|2010-05-12|113.6854|143.7779|169.3571|2.5328| 3.756|16.6513|101.8166|       NaN|                        6.3968|           0.0194|        1.9425|                       9319.5616|        265875.4219|                      15897.8723|             0.064|           2984.51|        8.2505|\n",
            "|2010-05-13|113.7009|143.8316|168.9141| 2.534|3.7911|16.6509|102.1318|       NaN|                        6.3981|           0.0194|        1.9363|                       9324.8575|        265862.9671|                      15916.5515|             0.064|           2985.64|        8.2523|\n",
            "|2010-05-14|113.6573| 142.566|166.1215|2.5333|3.7834|16.6476|101.8369|       NaN|                        6.3995|           0.0193|        1.9301|                       9330.1534|        265850.5123|                      15935.2307|             0.064|           2986.76|        8.2541|\n",
            "+----------+--------+--------+--------+------+------+-------+--------+----------+------------------------------+-----------------+--------------+--------------------------------+-------------------+--------------------------------+------------------+------------------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame(df_pd)\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QqTK4EBpxMrv"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year, to_date\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "df = df.withColumn(\"DATE\", to_date(df[\"DATE\"]))\n",
        "df = df.withColumn(\"YEAR\", year(\"DATE\"))\n",
        "\n",
        "features = [\n",
        "    \"SRI_LANKA_GDP_USD_BILLIONS_USD\",\n",
        "    \"ANNUAL_CHANGE_PCR\",\n",
        "    \"PER_CAPITA_USD\",\n",
        "    \"MERCHANDISE_EXPORTS_MILLIONS_USD\",\n",
        "    \"LABOUR_MIGRATION_SL\",\n",
        "    \"MERCHANDISE_IMPORTS_MILLIONS_USD\",\n",
        "    \"INFLATION_RATE_PCR\",\n",
        "    \"GDP_PER_CAPITA_USD\",\n",
        "    \"GDP_GROWTH_PCT\",\n",
        "    \"YEAR\"\n",
        "]\n",
        "\n",
        "targets = [\"USD\", \"EUR\", \"GBP\", \"INR\", \"RUB\", \"CNY\", \"AUD\"]\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"skip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC5A7Z40xR9x",
        "outputId": "b56c96b7-2cb5-4af2-9d0c-3440acd3d777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model for USD...\n",
            "USD RMSE: 2.9246\n",
            "Training model for EUR...\n",
            "EUR RMSE: 4.2860\n",
            "Training model for GBP...\n",
            "GBP RMSE: 4.5177\n",
            "Training model for INR...\n",
            "INR RMSE: 0.0428\n",
            "Training model for RUB...\n",
            "RUB RMSE: 0.1021\n",
            "Training model for CNY...\n",
            "CNY RMSE: 0.4702\n",
            "Training model for AUD...\n",
            "AUD RMSE: 2.8714\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "models = {}\n",
        "\n",
        "for target in targets:\n",
        "    print(f\"Training model for {target}...\")\n",
        "\n",
        "    data = assembler.transform(df).select(\"features\", target).na.drop()\n",
        "    train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    gbt = GBTRegressor(featuresCol=\"features\", labelCol=target, maxIter=100)\n",
        "    model = gbt.fit(train)\n",
        "    models[target] = model\n",
        "\n",
        "    predictions = model.transform(test)\n",
        "    evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "    rmse = evaluator.evaluate(predictions)\n",
        "    print(f\"{target} RMSE: {rmse:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jBzB2Qmc1DLA"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "def evaluate_model(model, test_df, label):\n",
        "    predictions = model.transform(test_df)\n",
        "\n",
        "    evaluators = {\n",
        "        \"RMSE\": RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"rmse\"),\n",
        "        \"MAE\": RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"mae\"),\n",
        "        \"R2\": RegressionEvaluator(labelCol=label, predictionCol=\"prediction\", metricName=\"r2\")\n",
        "    }\n",
        "\n",
        "    results = {metric: evaluator.evaluate(predictions) for metric, evaluator in evaluators.items()}\n",
        "\n",
        "    pred_pd = predictions.select(\"prediction\", label).toPandas()\n",
        "    pred_pd = pred_pd[(pred_pd[label] != 0)]\n",
        "    mape = (abs((pred_pd[label] - pred_pd[\"prediction\"]) / pred_pd[label])).mean() * 100\n",
        "    results[\"MAPE\"] = mape\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNQlYGpP1Hd6",
        "outputId": "9cda291e-0e27-404f-d5d0-cc69b260aea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîç Evaluating model for USD...\n",
            "RMSE: 2.9246\n",
            "MAE: 1.1119\n",
            "R2: 0.9987\n",
            "MAPE: 0.4404\n",
            "\n",
            "üîç Evaluating model for EUR...\n",
            "RMSE: 4.2860\n",
            "MAE: 2.1462\n",
            "R2: 0.9969\n",
            "MAPE: 0.8591\n",
            "\n",
            "üîç Evaluating model for GBP...\n",
            "RMSE: 4.5177\n",
            "MAE: 2.4650\n",
            "R2: 0.9973\n",
            "MAPE: 0.8330\n",
            "\n",
            "üîç Evaluating model for INR...\n",
            "RMSE: 0.0428\n",
            "MAE: 0.0210\n",
            "R2: 0.9968\n",
            "MAPE: 0.6996\n",
            "\n",
            "üîç Evaluating model for RUB...\n",
            "RMSE: 0.1021\n",
            "MAE: 0.0575\n",
            "R2: 0.9892\n",
            "MAPE: 1.7575\n",
            "\n",
            "üîç Evaluating model for CNY...\n",
            "RMSE: 0.4702\n",
            "MAE: 0.1980\n",
            "R2: 0.9981\n",
            "MAPE: 0.5541\n",
            "\n",
            "üîç Evaluating model for AUD...\n",
            "RMSE: 2.8714\n",
            "MAE: 1.5878\n",
            "R2: 0.9960\n",
            "MAPE: 1.0075\n"
          ]
        }
      ],
      "source": [
        "metrics = {}\n",
        "\n",
        "for target in targets:\n",
        "    print(f\"\\nüîç Evaluating model for {target}...\")\n",
        "    data = assembler.transform(df).select(\"features\", target).na.drop()\n",
        "    train, test = data.randomSplit([0.8, 0.2], seed=42)\n",
        "    model = models[target]\n",
        "\n",
        "    evaluation = evaluate_model(model, test, target)\n",
        "    metrics[target] = evaluation\n",
        "\n",
        "    for metric, value in evaluation.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epcV2NZF0nOq",
        "outputId": "2a54726f-a74a-4893-f654-6a8db5fce57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìà Predicted 2025 Exchange Rates:\n",
            "USD: 198.2020\n",
            "EUR: 242.0538\n",
            "GBP: 286.8468\n",
            "INR: 3.7375\n",
            "RUB: 2.8060\n",
            "CNY: 44.1299\n",
            "AUD: 202.7528\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "input_2025 = Row(\n",
        "    SRI_LANKA_GDP_USD_BILLIONS_USD=105,\n",
        "    ANNUAL_CHANGE_PCR=6.2,\n",
        "    PER_CAPITA_USD=4200,\n",
        "    MERCHANDISE_EXPORTS_MILLIONS_USD=12000,\n",
        "    LABOUR_MIGRATION_SL=270000,\n",
        "    MERCHANDISE_IMPORTS_MILLIONS_USD=20000,\n",
        "    INFLATION_RATE_PCR=6.0,\n",
        "    GDP_PER_CAPITA_USD=4400,\n",
        "    GDP_GROWTH_PCT=4.0,\n",
        "    YEAR=2025\n",
        ")\n",
        "\n",
        "predict_df = spark.createDataFrame([input_2025])\n",
        "vectorized_2025 = assembler.transform(predict_df)\n",
        "\n",
        "print(\"\\nüìà Predicted 2025 Exchange Rates:\")\n",
        "for currency, model in models.items():\n",
        "    result = model.transform(vectorized_2025)\n",
        "    pred_value = result.select(\"prediction\").collect()[0][0]\n",
        "    print(f\"{currency}: {pred_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3HNXbnQt36Ik"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "future_dates = pd.date_range(start=\"2025-03-19\", end=\"2026-12-31\", freq=\"D\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTyRQnuR38ee",
        "outputId": "2ae77003-1b29-487f-874d-0752a3740a4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-26-ada31a34bb79>:6: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  diffs = yearly_avg_pd[col].pct_change()\n"
          ]
        }
      ],
      "source": [
        "latest_vals = yearly_avg_pd[yearly_avg_pd[\"YEAR\"] == 2024].iloc[0].to_dict()\n",
        "base_values = {k: latest_vals[k] for k in latest_vals if k != \"YEAR\"}\n",
        "\n",
        "growth_rates = {}\n",
        "for col in base_values.keys():\n",
        "    diffs = yearly_avg_pd[col].pct_change()\n",
        "    avg_annual_growth = diffs.mean()\n",
        "    daily_growth = avg_annual_growth / 365\n",
        "    growth_rates[col] = daily_growth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NHivaiOe4Axg"
      },
      "outputs": [],
      "source": [
        "future_data = []\n",
        "\n",
        "for i, date in enumerate(future_dates):\n",
        "    row = {\"DATE\": date, \"YEAR\": date.year}\n",
        "    days_since_2024 = (date - pd.Timestamp(\"2024-01-01\")).days\n",
        "\n",
        "    for key in base_values:\n",
        "        growth = growth_rates.get(key, 0.0)\n",
        "        value = base_values[key] * ((1 + growth) ** days_since_2024)\n",
        "        row[key] = value\n",
        "\n",
        "    future_data.append(row)\n",
        "\n",
        "future_df_pd = pd.DataFrame(future_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tn85ORd24CkU"
      },
      "outputs": [],
      "source": [
        "future_df = spark.createDataFrame(future_df_pd)\n",
        "\n",
        "vectorized_future = assembler.transform(future_df)\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "final_df = vectorized_future.select(\"DATE\")\n",
        "\n",
        "for currency, model in models.items():\n",
        "    prediction = model.transform(vectorized_future).select(\"DATE\", col(\"prediction\").alias(currency))\n",
        "    final_df = final_df.join(prediction, on=\"DATE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8I2Qky94Gci",
        "outputId": "7a1e50e3-f91c-446d-d676-272e8f9bbe88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
            "|               DATE|               USD|               EUR|              GBP|               INR|              RUB|              CNY|              AUD|\n",
            "+-------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
            "|2025-03-19 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-20 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-21 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-22 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-23 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-24 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-25 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-26 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-27 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "|2025-03-28 00:00:00|302.26440697936664|334.58049324552786|388.7192455930673|3.7504533206474053|3.701708886486962|43.10822222069336|200.0568479665608|\n",
            "+-------------------+------------------+------------------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "final_result = final_df.orderBy(\"DATE\")\n",
        "final_result.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "de7ACsRm41aa"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "historical_rates = df.filter(col(\"DATE\") <= \"2025-03-18\") \\\n",
        "    .select(\"DATE\", *targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "hde7nP1X437p"
      },
      "outputs": [],
      "source": [
        "combined_rates = historical_rates.unionByName(final_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5wghHAWP5WAz"
      },
      "outputs": [],
      "source": [
        "combined_rates_pd = combined_rates.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yJQtyN_R5ZCS"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "\n",
        "engine = create_engine(\n",
        "    'snowflake://VIMUTHU04:Vimuthu20042007@dlbgnxt-iu64182/EXCHANGE_RATES_SL/DW?warehouse=COMPUTE_WH'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veawA5e05fyO"
      },
      "outputs": [],
      "source": [
        "sfOptions = {\n",
        "    \"sfURL\" : \"https://dlbgnxt-iu64182.snowflakecomputing.com\",\n",
        "    \"sfUser\" : \"###\",\n",
        "    \"sfPassword\" : \"###\",\n",
        "    \"sfDatabase\" : \"EXCHANGE_RATES_SL\",\n",
        "    \"sfSchema\" : \"DW\",\n",
        "    \"sfWarehouse\" : \"COMPUTE_WH\",\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br-J7Zqa5akm",
        "outputId": "5310fd50-c5c1-42ba-d72f-615925e96182"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-41-83952b03bc8a>:1: UserWarning: The provided table name 'PREDICTED_EXCHANGE_RATES_SL' is not found exactly as such in the database after writing the table, possibly due to case sensitivity issues. Consider using lower case table names.\n",
            "  combined_rates_pd.to_sql(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3783"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_rates_pd.to_sql(\n",
        "    name='PREDICTED_EXCHANGE_RATES_SL',\n",
        "    con=engine,\n",
        "    index=False,\n",
        "    if_exists='replace'\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
